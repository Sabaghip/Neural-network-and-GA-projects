{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OxtBoOczfZmr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummary import summary\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qw8Ke-OIzl4U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, NamedTuple, Callable, Optional, Union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "VHutq7nuxwmn"
      },
      "outputs": [],
      "source": [
        "##########Tensor################\n",
        "\n",
        "class Dependency(NamedTuple):\n",
        "    tensor: 'Tensor'\n",
        "    grad_fn: Callable[[np.ndarray], np.ndarray]\n",
        "\n",
        "\n",
        "Arrayable = Union[float, list, np.ndarray]\n",
        "\n",
        "def ensure_array(arrayable: Arrayable) -> np.ndarray:\n",
        "    if isinstance(arrayable, np.ndarray):\n",
        "        return arrayable\n",
        "    else:\n",
        "        return np.array(arrayable)\n",
        "\n",
        "Tensorable = Union[float, 'Tensor', np.ndarray]\n",
        "\n",
        "def ensure_tensor(tensorable: Tensorable) -> 'Tensor':\n",
        "    if isinstance(tensorable, Tensor):\n",
        "        return tensorable\n",
        "    else:\n",
        "        return Tensor(tensorable)\n",
        "\n",
        "class Tensor:\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            data : np.ndarray,\n",
        "            requires_grad: bool = False,\n",
        "            depends_on : List[Dependency] = None) -> None:\n",
        "\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data: value of tensor (numpy.ndarray)\n",
        "            requires_grad: if tensor needs grad (bool)\n",
        "            depends_on: list of dependencies\n",
        "        \"\"\"\n",
        "        self._data = ensure_array(data)\n",
        "        self.requires_grad = requires_grad\n",
        "        self.depends_on = depends_on\n",
        "\n",
        "        if not depends_on:\n",
        "            self.depends_on = []\n",
        "\n",
        "        self.shape = self._data.shape\n",
        "        self.grad : Optional['Tensor'] = None\n",
        "\n",
        "        if self.requires_grad:\n",
        "            self.zero_grad()\n",
        "\n",
        "    @property\n",
        "    def data(self):\n",
        "        return self._data\n",
        "\n",
        "    @data.setter\n",
        "    def data(self, new_data: np.ndarray) -> None:\n",
        "        self._data = new_data\n",
        "        self.grad = None\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        self.grad = Tensor(np.zeros_like(self.data, dtype=np.float64))\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Tensor({self.data}, requires_grad={self.requires_grad})\"\n",
        "\n",
        "    def sum(self) -> 'Tensor':\n",
        "        # TODO: implement sum over tensor elems\n",
        "        # Hint use _tensor_sum function\n",
        "        return _tensor_sum(self)\n",
        "\n",
        "    def log(self) -> 'Tensor':\n",
        "        # TODO: implement log\n",
        "        # Hint use _tensor_log function\n",
        "        return _tensor_log(self)\n",
        "\n",
        "    def exp(self) -> 'Tensor':\n",
        "        # TODO: implement exp\n",
        "        # Hint use _tensor_exp function\n",
        "        return _tensor_exp(self)\n",
        "\n",
        "    def __add__(self, other) -> 'Tensor':\n",
        "        # Done ( Don't change )\n",
        "        # Hint use _add function\n",
        "        # self + other\n",
        "        return _add(self, ensure_tensor(other))\n",
        "\n",
        "    def __radd__(self, other) -> 'Tensor':\n",
        "        # TODO: implement radd\n",
        "        # Hint use _add function\n",
        "        # other + self\n",
        "        return _add(ensure_tensor(other), self)\n",
        "\n",
        "    def __iadd__(self, other) -> 'Tensor':\n",
        "        # TODO: implement inc add\n",
        "        # Hint use _add function\n",
        "        # self += other\n",
        "        new_tensor = self + other\n",
        "        self = Tensor(new_tensor.data, requires_grad=new_tensor.requires_grad, depends_on=new_tensor.depends_on)\n",
        "        return self\n",
        "\n",
        "    def __sub__(self, other) -> 'Tensor':\n",
        "        # TODO: implement sub\n",
        "        # Hint use _sub function\n",
        "        # self - other\n",
        "        return _sub(self, ensure_tensor(other))\n",
        "\n",
        "    def __rsub__(self, other) -> 'Tensor':\n",
        "        # TODO: implement rsub\n",
        "        # Hint use _sub function\n",
        "        # other - self\n",
        "        return _sub(ensure_tensor(other), self)\n",
        "\n",
        "    def __isub__(self, other) -> 'Tensor':\n",
        "        # TODO: implement inc sub\n",
        "        # Hint use _sub function\n",
        "        # self -= other\n",
        "        new_tensor = self - other\n",
        "        self = Tensor(new_tensor.data, requires_grad=new_tensor.requires_grad, depends_on=new_tensor.depends_on)\n",
        "        return self\n",
        "\n",
        "    def __mul__(self, other) -> 'Tensor':\n",
        "        # TODO: implement elemnet-wise mul\n",
        "        # Hint use _mul function\n",
        "        # self * other\n",
        "        return _mul(self, ensure_tensor(other))\n",
        "\n",
        "    def __rmul__(self, other) -> 'Tensor':\n",
        "        # TODO: implement elemnet-wise rmul\n",
        "        # Hint use _mul function\n",
        "        # other * self\n",
        "        return _mul(ensure_tensor(other), self)\n",
        "\n",
        "    def __imul__(self, other) -> 'Tensor':\n",
        "        # TODO: implement elemnet-wise inc mul\n",
        "        # Hint use _mul function\n",
        "        # self *= other\n",
        "        new_tensor = self * other\n",
        "        self = Tensor(new_tensor.data, requires_grad=new_tensor.requires_grad, depends_on=new_tensor.depends_on)\n",
        "        return self\n",
        "\n",
        "    def __matmul__(self, other) -> 'Tensor':\n",
        "        # TODO: implement matrix mul\n",
        "        # Hint use _matmul function\n",
        "        # self @ other\n",
        "        return _matmul(self, ensure_tensor(other))\n",
        "\n",
        "    def __pow__(self, power: float):\n",
        "        # TODO: implement power\n",
        "        # Hint use _tensor_pow function\n",
        "        # self ** power\n",
        "        return _tensor_pow(self, power)\n",
        "\n",
        "    def __getitem__(self, idcs):\n",
        "        # TODO: implement getitem [:]\n",
        "        # Hint use _tensor_slice function\n",
        "        return _tensor_slice(self, idcs)\n",
        "\n",
        "    def __neg__(self, idcs):\n",
        "        # TODO: implement neg (-)\n",
        "        # Hint use -_tensor_neg function\n",
        "        neg = _tensor_neg(self)\n",
        "        return _tensor_slice(neg, idcs)\n",
        "\n",
        "\n",
        "\n",
        "    def backward(self, grad: 'Tensor' = None) -> None:\n",
        "        if grad is None:\n",
        "            if self.shape == ():\n",
        "                grad = Tensor(1.0)\n",
        "            else:\n",
        "                raise RuntimeError(\"grad must be specified for non-0-tensor\")\n",
        "        self.grad.data = self.grad.data + grad.data\n",
        "        for dependency in self.depends_on:\n",
        "            backward_grad = dependency.grad_fn(grad.data)\n",
        "            dependency.tensor.backward(Tensor(backward_grad))\n",
        "\n",
        "\n",
        "def _tensor_sum(t: Tensor) -> Tensor:\n",
        "    data = t.data.sum()\n",
        "    req_grad = t.requires_grad\n",
        "\n",
        "    if req_grad:\n",
        "        def grad_fn(grad: np.ndarray):\n",
        "            return grad * np.ones_like(t.data)\n",
        "\n",
        "        depends_on = [Dependency(t, grad_fn)]\n",
        "\n",
        "    else:\n",
        "        depends_on = []\n",
        "\n",
        "    return Tensor(data=data, requires_grad=req_grad, depends_on=depends_on)\n",
        "\n",
        "def _tensor_log(t: Tensor) -> Tensor:\n",
        "    # TODO\n",
        "    data = np.log(t.data)\n",
        "    req_grad = t.requires_grad\n",
        "\n",
        "    if req_grad:\n",
        "        def grad_fn(grad: np.ndarray):\n",
        "            return grad * (1 / t.data) * np.ones_like(t.data)\n",
        "\n",
        "        depends_on = [Dependency(t, grad_fn)]\n",
        "\n",
        "    else:\n",
        "        depends_on = []\n",
        "\n",
        "    return Tensor(data=data, requires_grad=req_grad, depends_on=depends_on)\n",
        "\n",
        "def _tensor_exp(t: Tensor) -> Tensor:\n",
        "    # TODO\n",
        "    # print(\"oooooooooo\",t.data,\"ppppppppppppppp\")\n",
        "    data = np.exp(t.data)\n",
        "    req_grad = t.requires_grad\n",
        "\n",
        "    if req_grad:\n",
        "        def grad_fn(grad: np.ndarray):\n",
        "            return grad * data * np.ones_like(t.data)\n",
        "\n",
        "        depends_on = [Dependency(t, grad_fn)]\n",
        "\n",
        "    else:\n",
        "        depends_on = []\n",
        "\n",
        "    return Tensor(data=data, requires_grad=req_grad, depends_on=depends_on)\n",
        "\n",
        "def _tensor_pow(t: Tensor, power:float) -> Tensor:\n",
        "    # TODO\n",
        "    data = np.power(t.data, power)\n",
        "    req_grad = t.requires_grad\n",
        "\n",
        "    if req_grad:\n",
        "        def grad_fn(grad: np.ndarray):\n",
        "            return grad * power * np.power(t.data, power - 1)\n",
        "\n",
        "        depends_on = [Dependency(t, grad_fn)]\n",
        "\n",
        "    else:\n",
        "        depends_on = []\n",
        "\n",
        "    return Tensor(data=data, requires_grad=req_grad, depends_on=depends_on)\n",
        "\n",
        "def _tensor_slice(t: Tensor, idcs) -> Tensor:\n",
        "    # TODO\n",
        "    data = t.data[idcs]\n",
        "    requires_grad = t.requires_grad\n",
        "\n",
        "    if requires_grad:\n",
        "        def grad_fn(grad: np.ndarray) -> np.ndarray:\n",
        "            bigger_grad = np.zeros_like(data)\n",
        "            bigger_grad[idcs] = grad\n",
        "            return bigger_grad\n",
        "\n",
        "        depends_on = Dependency(t, grad_fn)\n",
        "    else:\n",
        "        depends_on = []\n",
        "\n",
        "    return Tensor(data, requires_grad, depends_on)\n",
        "\n",
        "def _tensor_neg(t: Tensor) -> Tensor:\n",
        "    # TODO\n",
        "    data = np.negative(t.data)\n",
        "    requires_grad = t.requires_grad\n",
        "    if requires_grad:\n",
        "        depends_on = [Dependency(t, lambda x: -x)]\n",
        "    else:\n",
        "        depends_on = []\n",
        "\n",
        "    return Tensor(data, requires_grad, depends_on)\n",
        "\n",
        "def _add(t1: Tensor, t2: Tensor) -> Tensor:\n",
        "\n",
        "    data = t1.data + t2.data\n",
        "    req_grad = t1.requires_grad or t2.requires_grad\n",
        "    depends_on : List[Dependency] = []\n",
        "\n",
        "    if t1.requires_grad:\n",
        "        def grad_fn1(grad: np.ndarray) -> np.ndarray:\n",
        "            ndims_added = grad.ndim - t1.data.ndim\n",
        "            for _ in range(ndims_added):\n",
        "                grad = grad.sum(axis=0)\n",
        "            for i, dim in enumerate(t1.shape):\n",
        "                if dim == 1:\n",
        "                    grad = grad.sum(axis=i, keepdims=True)\n",
        "            return grad\n",
        "        depends_on.append(Dependency(t1, grad_fn1))\n",
        "\n",
        "    if t2.requires_grad:\n",
        "        def grad_fn2(grad: np.ndarray) -> np.ndarray:\n",
        "            ndims_added = grad.ndim - t2.data.ndim\n",
        "            for _ in range(ndims_added):\n",
        "                grad = grad.sum(axis=0)\n",
        "            for i, dim in enumerate(t2.shape):\n",
        "                if dim == 1:\n",
        "                    grad = grad.sum(axis=i, keepdims=True)\n",
        "            return grad\n",
        "        depends_on.append(Dependency(t2, grad_fn2))\n",
        "\n",
        "    return Tensor(\n",
        "        data=data,\n",
        "        requires_grad=req_grad,\n",
        "        depends_on=depends_on\n",
        "    )\n",
        "\n",
        "def _sub(t1: Tensor, t2: Tensor) -> Tensor:\n",
        "    # TODO: implement sub\n",
        "    # Hint: a-b = a+(-b)\n",
        "    return _add(t1, _tensor_neg(t2))\n",
        "\n",
        "def _mul(t1: Tensor, t2: Tensor) -> Tensor:\n",
        "    # Done ( Don't change )\n",
        "    data = t1.data * t2.data\n",
        "    req_grad = t1.requires_grad or t2.requires_grad\n",
        "    depends_on : List[Dependency] = []\n",
        "\n",
        "    if t1.requires_grad:\n",
        "        def grad_fn1(grad: np.ndarray) -> np.ndarray:\n",
        "            grad = grad * t2.data\n",
        "            ndims_added = grad.ndim - t1.data.ndim\n",
        "            for _ in range(ndims_added):\n",
        "                grad = grad.sum(axis=0)\n",
        "            for i, dim in enumerate(t1.shape):\n",
        "                if dim == 1:\n",
        "                    grad = grad.sum(axis=i, keepdims=True)\n",
        "            return grad\n",
        "        depends_on.append(Dependency(t1, grad_fn1))\n",
        "    if t2.requires_grad:\n",
        "        def grad_fn2(grad: np.ndarray) -> np.ndarray:\n",
        "            grad = grad * t1.data\n",
        "            ndims_added = grad.ndim - t2.data.ndim\n",
        "            for _ in range(ndims_added):\n",
        "                grad = grad.sum(axis=0)\n",
        "            for i, dim in enumerate(t2.shape):\n",
        "                if dim == 1:\n",
        "                    grad = grad.sum(axis=i, keepdims=True)\n",
        "            return grad\n",
        "        depends_on.append(Dependency(t2, grad_fn2))\n",
        "\n",
        "    return Tensor(\n",
        "        data=data,\n",
        "        requires_grad=req_grad,\n",
        "        depends_on=depends_on\n",
        "    )\n",
        "\n",
        "def _matmul(t1: Tensor, t2: Tensor) -> Tensor:\n",
        "    # TODO: implement matrix multiplication\n",
        "    data = t1.data @ t2.data\n",
        "    requires_grad = t1.requires_grad or t2.requires_grad\n",
        "\n",
        "    depends_on: List[Dependency] = []\n",
        "\n",
        "    if t1.requires_grad:\n",
        "        def grad_fn1(grad: np.ndarray) -> np.ndarray:\n",
        "            return grad @ t2.data.T\n",
        "        depends_on.append(Dependency(t1, grad_fn1))\n",
        "\n",
        "    if t2.requires_grad:\n",
        "        def grad_fn2(grad: np.ndarray) -> np.ndarray:\n",
        "            return t1.data.T @ grad\n",
        "        depends_on.append(Dependency(t2, grad_fn2))\n",
        "\n",
        "    return Tensor(data,\n",
        "                  requires_grad,\n",
        "                  depends_on)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "PKeCnlLGzow6"
      },
      "outputs": [],
      "source": [
        "#####activation######\n",
        "\n",
        "def Sigmoid(t: Tensor) -> Tensor:\n",
        "    # TODO: implement sigmoid function\n",
        "    # hint: you can do it using function you've implemented (not directly define grad func)\n",
        "    return Tensor(data=1 / (1 + (-t).exp()), requires_grad=t.requires_grad, depends_on=t.depends_on)\n",
        "\n",
        "def Tanh(t: Tensor) -> Tensor:\n",
        "    # TODO: implement tanh function\n",
        "    # hint: you can do it using function you've implemented (not directly define grad func)\n",
        "    return Tensor(data=(t.exp() - (-t).exp())/(t.exp() + (-t).exp()), requires_grad=t.requires_grad, depends_on=t.depends_on)\n",
        "\n",
        "def Softmax(t: Tensor) -> Tensor:\n",
        "    # TODO: implement softmax function\n",
        "    # hint: you can do it using function you've implemented (not directly define grad func)\n",
        "    # hint: you can't use sum because it has not axis argument so there are 2 ways:\n",
        "    #        1. implement sum by axis\n",
        "    #        2. using matrix mul to do it :) (recommended)\n",
        "    # hint: a/b = a*(b^-1)\n",
        "    sum = t.exp() @ np.ones((t.data.shape[1], 1))\n",
        "    # print(\"vvvvvvvvvvvvvvv\",sum.data, \"qqqqqqqqqqqqqqqqq\")\n",
        "    return t.exp() * Tensor(1/sum.data)\n",
        "\n",
        "def Relu(t: Tensor) -> Tensor:\n",
        "    # TODO: implement relu function\n",
        "\n",
        "    # use np.maximum\n",
        "    data = np.maximum(np.zeros_like(t.data), t.data)\n",
        "\n",
        "    req_grad = t.requires_grad\n",
        "    if req_grad:\n",
        "        def grad_fn(grad: np.ndarray):\n",
        "            # use np.where\n",
        "            return np.where(t.data > 0, grad,np.zeros_like(grad))\n",
        "\n",
        "        depends_on = [Dependency(t, grad_fn)]\n",
        "    else:\n",
        "        depends_on = []\n",
        "    return Tensor(data=data, requires_grad=req_grad, depends_on=depends_on)\n",
        "\n",
        "\n",
        "def LeakyRelu(t: Tensor,leak=0.05) -> Tensor:\n",
        "    \"\"\"\n",
        "    fill 'data' and 'req_grad' and implement LeakyRelu grad_fn\n",
        "    hint: use np.where like Relu method but for LeakyRelu\n",
        "    \"\"\"\n",
        "    # TODO: implement leaky_relu function\n",
        "\n",
        "    data = np.where(t.data > 0, t.data, leak * t.data)\n",
        "\n",
        "    req_grad = t.requires_grad\n",
        "    if req_grad:\n",
        "        def grad_fn(grad: np.ndarray):\n",
        "            return np.where(t.data > 0, grad, leak * grad)\n",
        "\n",
        "        depends_on = [Dependency(t, grad_fn)]\n",
        "    else:\n",
        "        depends_on = []\n",
        "\n",
        "    return Tensor(data=data, requires_grad=req_grad, depends_on=depends_on)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "TakzEDR8z0_i"
      },
      "outputs": [],
      "source": [
        "##########initializers############\n",
        "# TODO: implement xavier_initializer, zero_initializer\n",
        "\n",
        "def xavier_initializer(shape):\n",
        "    return np.random.randn(*shape) * np.sqrt(1/shape[0], dtype=np.float64)\n",
        "\n",
        "def he_initializer(shape):\n",
        "    return np.random.randn(*shape) * np.sqrt(2/shape[0], dtype=np.float64)\n",
        "\n",
        "\n",
        "def zero_initializer(shape):\n",
        "    return np.zeros(shape, dtype=np.float64)\n",
        "\n",
        "def one_initializer(shape):\n",
        "    return np.ones(shape, dtype=np.float64)\n",
        "\n",
        "def initializer(shape, mode=\"xavier\"):\n",
        "    if mode == \"xavier\":\n",
        "        return xavier_initializer(shape)\n",
        "    elif mode == \"he\":\n",
        "        return he_initializer(shape)\n",
        "    elif mode == \"zero\":\n",
        "        return zero_initializer(shape)\n",
        "    elif mode == \"one\":\n",
        "        return one_initializer(shape)\n",
        "    else:\n",
        "        raise NotImplementedError(\"Not implemented initializer method\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "InFFl6aG0BcJ"
      },
      "outputs": [],
      "source": [
        "# @title Default title text\n",
        "##########fc_layer############\n",
        "class Linear:\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, need_bias=True, mode='zero') -> None:\n",
        "        # set input and output shape of layer\n",
        "        self.shape = (in_channels, out_channels)\n",
        "        self.need_bias = need_bias\n",
        "        # TODO initialize weight by initializer function (mode)\n",
        "        self.weight = Tensor(\n",
        "            data=initializer((in_channels, out_channels), mode=mode),\n",
        "            requires_grad=True\n",
        "        )\n",
        "        # TODO initialize weight by initializer function (zero mode)\n",
        "        if self.need_bias:\n",
        "            self.bias = Tensor(\n",
        "                data=0,\n",
        "                requires_grad=True\n",
        "            )\n",
        "\n",
        "    def forward(self, inp):\n",
        "      # TODO:implement forward propagation\n",
        "      if self.need_bias:\n",
        "          return inp.__matmul__(self.weight) + self.bias\n",
        "      return inp.__matmul__(self.weight)\n",
        "\n",
        "    def parameters(self):\n",
        "        if self.need_bias:\n",
        "            return [self. weight, self.bias]\n",
        "        return [self. weight]\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.weight.zero_grad()\n",
        "        if self.need_bias:\n",
        "            self.bias.zero_grad()\n",
        "\n",
        "    def __call__(self, inp):\n",
        "        return self.forward(inp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "PWBthNyF0LCh"
      },
      "outputs": [],
      "source": [
        "##########loss############\n",
        "\n",
        "def MeanSquaredError(preds: Tensor, actual: Tensor):\n",
        "    # TODO : implement mean squared error\n",
        "    err = ((actual - preds) * (actual - preds)).sum()\n",
        "    res = err * (Tensor(1 / len(actual.data), requires_grad=True, depends_on=actual.depends_on))\n",
        "    return res\n",
        "\n",
        "def CategoricalCrossEntropy(preds: Tensor, actual: Tensor):\n",
        "    # TODO : imlement categorical cross entropy\n",
        "    sum = 0\n",
        "    for i in range(len(preds.data)):\n",
        "        sum -= actual.data[i] * np.log(preds.data[i])\n",
        "    return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "GSYwgmYF0ivQ"
      },
      "outputs": [],
      "source": [
        "##########optim############\n",
        "class Optimizer:\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for l in self.layers:\n",
        "            l.zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "hC766jxH0Uuh"
      },
      "outputs": [],
      "source": [
        "##########sgd############\n",
        "# TODO: implement step function\n",
        "class SGD(Optimizer):\n",
        "    def __init__(self, layers, learning_rate=0.1):\n",
        "        super().__init__(layers)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def step(self):\n",
        "        # TODO: update weight and biases ( Don't use '-=' and use l.weight = l.weight - ... )\n",
        "        for l in self.layers:\n",
        "            temp = l.parameters()\n",
        "            params = temp[0]\n",
        "            bias = temp[1]\n",
        "            l.weight = l.weight - self.learning_rate * params.grad\n",
        "            if l.need_bias:\n",
        "                l.bias = l.bias - self.learning_rate * bias.grad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "cAceiLFbwHOh"
      },
      "outputs": [],
      "source": [
        "# TODO: in this task you have to\n",
        "# 1. load mnist dataset for our framework\n",
        "transform=transforms.Compose([\n",
        "        ToTensor(),\n",
        "        transforms.Normalize(0,1)\n",
        "        ])\n",
        "train_set = datasets.MNIST('./Datasets', download=True, train=True, transform=transform)\n",
        "\n",
        "test_set = datasets.MNIST('./Datasets', download=True, train=False, transform=transform)\n",
        "\n",
        "# train_set = datasets.MNIST('./Datasets', download=True, train=True, transform=transform)\n",
        "train = Tensor([i[0][0].data.numpy() for i in train_set])\n",
        "train_label = Tensor([i[1] for i in train_set])\n",
        "test = Tensor([i[0][0].data.numpy() for i in test_set])\n",
        "test_label = Tensor([i[1] for i in test_set])\n",
        "# x = Tensor(train_set[1][0][0].data, requires_grad=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "MKOTgLpU2tO9"
      },
      "outputs": [],
      "source": [
        "# 2. define your model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        # self.fc1 = Linear(28 * 28, 256)\n",
        "        # self.fc2 = Linear(256, 10)\n",
        "        self.fc3 = Linear(28 * 28, 10)\n",
        "        self.relu = Relu\n",
        "        self.softmax = Softmax\n",
        "        # TODO: define layers of your model\n",
        "    def forward(self, x):\n",
        "        x = Tensor(x.data.reshape(-1,28*28))\n",
        "        # x = self.fc1(x)\n",
        "        # x = self.relu(x)\n",
        "        # x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        # TODO: define forward for your model\n",
        "        return self.softmax(x)\n",
        "\n",
        "\n",
        "model = Model()\n",
        "\n",
        "learning_rate = 0.007\n",
        "optimizer = SGD([model.fc3,], learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7q3yT39wWIn",
        "outputId": "a3868f32-9cc9-46b3-80c5-6b693f7f3631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor([[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]], requires_grad=True)\n",
            "Tensor(0, requires_grad=True)\n",
            "0 5\n",
            "5 10\n",
            "10 15\n",
            "15 20\n",
            "20 25\n",
            "25 30\n",
            "30 35\n",
            "35 40\n",
            "40 45\n",
            "45 50\n",
            "50 55\n",
            "55 60\n",
            "60 65\n",
            "65 70\n",
            "70 75\n",
            "75 80\n",
            "80 85\n",
            "85 90\n",
            "90 95\n",
            "95 100\n",
            "100 105\n",
            "105 110\n",
            "110 115\n",
            "115 120\n",
            "120 125\n",
            "125 130\n",
            "130 135\n",
            "135 140\n",
            "140 145\n",
            "145 150\n",
            "150 155\n",
            "155 160\n",
            "160 165\n",
            "165 170\n",
            "170 175\n",
            "175 180\n",
            "180 185\n",
            "185 190\n",
            "190 195\n",
            "195 200\n",
            "200 205\n",
            "205 210\n",
            "210 215\n",
            "215 220\n",
            "220 225\n",
            "225 230\n",
            "230 235\n",
            "235 240\n",
            "240 245\n",
            "245 250\n",
            "250 255\n",
            "255 260\n",
            "260 265\n",
            "265 270\n",
            "270 275\n",
            "275 280\n",
            "280 285\n",
            "285 290\n",
            "290 295\n",
            "295 300\n",
            "300 305\n",
            "305 310\n",
            "310 315\n",
            "315 320\n",
            "320 325\n",
            "325 330\n",
            "330 335\n",
            "335 340\n",
            "340 345\n",
            "345 350\n",
            "350 355\n",
            "355 360\n",
            "360 365\n",
            "365 370\n",
            "370 375\n",
            "375 380\n",
            "380 385\n",
            "385 390\n",
            "390 395\n",
            "395 400\n",
            "400 405\n",
            "405 410\n",
            "410 415\n",
            "415 420\n",
            "420 425\n",
            "425 430\n",
            "430 435\n",
            "435 440\n",
            "440 445\n",
            "445 450\n",
            "450 455\n",
            "455 460\n",
            "460 465\n",
            "465 470\n",
            "470 475\n",
            "475 480\n",
            "480 485\n",
            "485 490\n",
            "490 495\n",
            "495 500\n",
            "500 505\n",
            "505 510\n",
            "510 515\n",
            "515 520\n",
            "520 525\n",
            "525 530\n",
            "530 535\n",
            "535 540\n",
            "540 545\n",
            "545 550\n",
            "550 555\n",
            "555 560\n",
            "560 565\n",
            "565 570\n",
            "570 575\n",
            "575 580\n",
            "580 585\n",
            "585 590\n",
            "590 595\n",
            "595 600\n",
            "600 605\n",
            "605 610\n",
            "610 615\n",
            "615 620\n",
            "620 625\n",
            "625 630\n",
            "630 635\n",
            "635 640\n",
            "640 645\n",
            "645 650\n",
            "650 655\n",
            "655 660\n",
            "660 665\n",
            "665 670\n",
            "670 675\n",
            "675 680\n",
            "680 685\n",
            "685 690\n",
            "690 695\n",
            "695 700\n",
            "700 705\n",
            "705 710\n",
            "710 715\n",
            "715 720\n",
            "720 725\n",
            "725 730\n",
            "730 735\n",
            "735 740\n",
            "740 745\n",
            "745 750\n",
            "750 755\n",
            "755 760\n",
            "760 765\n",
            "765 770\n",
            "770 775\n",
            "775 780\n",
            "780 785\n",
            "785 790\n",
            "790 795\n",
            "795 800\n",
            "800 805\n",
            "805 810\n",
            "810 815\n",
            "815 820\n",
            "820 825\n",
            "825 830\n",
            "830 835\n",
            "835 840\n",
            "840 845\n",
            "845 850\n",
            "850 855\n",
            "855 860\n",
            "860 865\n",
            "865 870\n",
            "870 875\n",
            "875 880\n",
            "880 885\n",
            "885 890\n",
            "890 895\n",
            "895 900\n",
            "900 905\n",
            "905 910\n",
            "910 915\n",
            "915 920\n",
            "920 925\n",
            "925 930\n",
            "930 935\n",
            "935 940\n",
            "940 945\n",
            "945 950\n",
            "950 955\n",
            "955 960\n",
            "960 965\n",
            "965 970\n",
            "970 975\n",
            "975 980\n",
            "980 985\n",
            "985 990\n",
            "990 995\n",
            "995 1000\n",
            "1000 1005\n",
            "1005 1010\n",
            "1010 1015\n",
            "1015 1020\n",
            "1020 1025\n",
            "1025 1030\n",
            "1030 1035\n",
            "1035 1040\n",
            "1040 1045\n",
            "1045 1050\n",
            "1050 1055\n",
            "1055 1060\n",
            "1060 1065\n",
            "1065 1070\n",
            "1070 1075\n",
            "1075 1080\n",
            "1080 1085\n",
            "1085 1090\n",
            "1090 1095\n",
            "1095 1100\n",
            "1100 1105\n",
            "1105 1110\n",
            "1110 1115\n",
            "1115 1120\n",
            "1120 1125\n",
            "1125 1130\n",
            "1130 1135\n",
            "1135 1140\n",
            "1140 1145\n",
            "1145 1150\n",
            "1150 1155\n",
            "1155 1160\n",
            "1160 1165\n",
            "1165 1170\n",
            "1170 1175\n",
            "1175 1180\n",
            "1180 1185\n",
            "1185 1190\n",
            "1190 1195\n",
            "1195 1200\n",
            "1200 1205\n",
            "1205 1210\n",
            "1210 1215\n",
            "1215 1220\n",
            "1220 1225\n",
            "1225 1230\n",
            "1230 1235\n",
            "1235 1240\n",
            "1240 1245\n",
            "1245 1250\n",
            "1250 1255\n",
            "1255 1260\n",
            "1260 1265\n",
            "1265 1270\n",
            "1270 1275\n",
            "1275 1280\n",
            "1280 1285\n",
            "1285 1290\n",
            "1290 1295\n",
            "1295 1300\n",
            "1300 1305\n",
            "1305 1310\n",
            "1310 1315\n",
            "1315 1320\n",
            "1320 1325\n",
            "1325 1330\n",
            "1330 1335\n",
            "1335 1340\n",
            "1340 1345\n",
            "1345 1350\n",
            "1350 1355\n",
            "1355 1360\n",
            "1360 1365\n",
            "1365 1370\n",
            "1370 1375\n",
            "1375 1380\n",
            "1380 1385\n",
            "1385 1390\n",
            "1390 1395\n",
            "1395 1400\n",
            "1400 1405\n",
            "1405 1410\n",
            "1410 1415\n",
            "1415 1420\n",
            "1420 1425\n",
            "1425 1430\n",
            "1430 1435\n",
            "1435 1440\n",
            "1440 1445\n",
            "1445 1450\n",
            "1450 1455\n",
            "1455 1460\n",
            "1460 1465\n",
            "1465 1470\n",
            "1470 1475\n",
            "1475 1480\n",
            "1480 1485\n",
            "1485 1490\n",
            "1490 1495\n",
            "1495 1500\n",
            "1500 1505\n",
            "1505 1510\n",
            "1510 1515\n",
            "1515 1520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x7fcf52c0d480>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 97, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1520 1525\n",
            "1525 1530\n",
            "1530 1535\n",
            "1535 1540\n",
            "1540 1545\n",
            "1545 1550\n",
            "1550 1555\n",
            "1555 1560\n",
            "1560 1565\n",
            "1565 1570\n",
            "1570 1575\n",
            "1575 1580\n",
            "1580 1585\n",
            "1585 1590\n",
            "1590 1595\n",
            "1595 1600\n",
            "1600 1605\n",
            "1605 1610\n",
            "1610 1615\n",
            "1615 1620\n",
            "1620 1625\n",
            "1625 1630\n",
            "1630 1635\n",
            "1635 1640\n",
            "1640 1645\n",
            "1645 1650\n",
            "1650 1655\n",
            "1655 1660\n",
            "1660 1665\n",
            "1665 1670\n",
            "1670 1675\n",
            "1675 1680\n",
            "1680 1685\n",
            "1685 1690\n",
            "1690 1695\n",
            "1695 1700\n",
            "1700 1705\n",
            "1705 1710\n",
            "1710 1715\n",
            "1715 1720\n",
            "1720 1725\n",
            "1725 1730\n",
            "1730 1735\n",
            "1735 1740\n",
            "1740 1745\n",
            "1745 1750\n",
            "1750 1755\n",
            "1755 1760\n",
            "1760 1765\n",
            "1765 1770\n",
            "1770 1775\n",
            "1775 1780\n",
            "1780 1785\n",
            "1785 1790\n",
            "1790 1795\n",
            "1795 1800\n",
            "1800 1805\n",
            "1805 1810\n",
            "1810 1815\n",
            "1815 1820\n",
            "1820 1825\n",
            "1825 1830\n",
            "1830 1835\n",
            "1835 1840\n",
            "1840 1845\n",
            "1845 1850\n",
            "1850 1855\n",
            "1855 1860\n",
            "1860 1865\n",
            "1865 1870\n",
            "1870 1875\n",
            "1875 1880\n",
            "1880 1885\n",
            "1885 1890\n",
            "1890 1895\n",
            "1895 1900\n",
            "1900 1905\n",
            "1905 1910\n",
            "1910 1915\n",
            "1915 1920\n",
            "1920 1925\n",
            "1925 1930\n",
            "1930 1935\n",
            "1935 1940\n",
            "1940 1945\n",
            "1945 1950\n",
            "1950 1955\n",
            "1955 1960\n",
            "1960 1965\n",
            "1965 1970\n",
            "1970 1975\n",
            "1975 1980\n",
            "1980 1985\n",
            "1985 1990\n",
            "1990 1995\n",
            "1995 2000\n",
            "2000 2005\n",
            "2005 2010\n",
            "2010 2015\n",
            "2015 2020\n",
            "2020 2025\n",
            "2025 2030\n",
            "2030 2035\n",
            "2035 2040\n",
            "2040 2045\n",
            "2045 2050\n",
            "2050 2055\n",
            "2055 2060\n",
            "2060 2065\n",
            "2065 2070\n",
            "2070 2075\n",
            "2075 2080\n",
            "2080 2085\n",
            "2085 2090\n",
            "2090 2095\n",
            "2095 2100\n",
            "2100 2105\n",
            "2105 2110\n",
            "2110 2115\n",
            "2115 2120\n",
            "2120 2125\n",
            "2125 2130\n",
            "2130 2135\n",
            "2135 2140\n",
            "2140 2145\n",
            "2145 2150\n",
            "2150 2155\n",
            "2155 2160\n",
            "2160 2165\n",
            "2165 2170\n",
            "2170 2175\n",
            "2175 2180\n",
            "2180 2185\n",
            "2185 2190\n",
            "2190 2195\n",
            "2195 2200\n",
            "2200 2205\n",
            "2205 2210\n",
            "2210 2215\n",
            "2215 2220\n",
            "2220 2225\n",
            "2225 2230\n",
            "2230 2235\n",
            "2235 2240\n",
            "2240 2245\n",
            "2245 2250\n",
            "2250 2255\n",
            "2255 2260\n",
            "2260 2265\n",
            "2265 2270\n",
            "2270 2275\n",
            "2275 2280\n",
            "2280 2285\n",
            "2285 2290\n",
            "2290 2295\n",
            "2295 2300\n",
            "2300 2305\n",
            "2305 2310\n",
            "2310 2315\n",
            "2315 2320\n",
            "2320 2325\n",
            "2325 2330\n",
            "2330 2335\n",
            "2335 2340\n",
            "2340 2345\n",
            "2345 2350\n",
            "2350 2355\n",
            "2355 2360\n",
            "2360 2365\n",
            "2365 2370\n",
            "2370 2375\n",
            "2375 2380\n",
            "2380 2385\n",
            "2385 2390\n",
            "2390 2395\n",
            "2395 2400\n",
            "2400 2405\n",
            "2405 2410\n",
            "2410 2415\n",
            "2415 2420\n",
            "2420 2425\n",
            "2425 2430\n",
            "2430 2435\n",
            "2435 2440\n",
            "2440 2445\n",
            "2445 2450\n",
            "2450 2455\n",
            "2455 2460\n",
            "2460 2465\n",
            "2465 2470\n",
            "2470 2475\n",
            "2475 2480\n",
            "2480 2485\n",
            "2485 2490\n",
            "2490 2495\n",
            "2495 2500\n",
            "2500 2505\n",
            "2505 2510\n",
            "2510 2515\n",
            "2515 2520\n",
            "2520 2525\n",
            "2525 2530\n",
            "2530 2535\n",
            "2535 2540\n",
            "2540 2545\n",
            "2545 2550\n",
            "2550 2555\n",
            "2555 2560\n",
            "2560 2565\n",
            "2565 2570\n",
            "2570 2575\n",
            "2575 2580\n",
            "2580 2585\n",
            "2585 2590\n",
            "2590 2595\n",
            "2595 2600\n",
            "2600 2605\n",
            "2605 2610\n",
            "2610 2615\n",
            "2615 2620\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-97-beef4da9bff9>:212: RuntimeWarning: overflow encountered in exp\n",
            "  data = np.exp(t.data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2620 2625\n",
            "2625 2630\n",
            "2630 2635\n",
            "2635 2640\n",
            "2640 2645\n",
            "2645 2650\n",
            "2650 2655\n",
            "2655 2660\n",
            "2660 2665\n",
            "2665 2670\n",
            "2670 2675\n",
            "2675 2680\n",
            "2680 2685\n",
            "2685 2690\n",
            "2690 2695\n",
            "2695 2700\n",
            "2700 2705\n",
            "2705 2710\n",
            "2710 2715\n",
            "2715 2720\n",
            "2720 2725\n",
            "2725 2730\n",
            "2730 2735\n",
            "2735 2740\n",
            "2740 2745\n",
            "2745 2750\n",
            "2750 2755\n",
            "2755 2760\n",
            "2760 2765\n",
            "2765 2770\n",
            "2770 2775\n",
            "2775 2780\n",
            "2780 2785\n",
            "2785 2790\n",
            "2790 2795\n",
            "2795 2800\n",
            "2800 2805\n",
            "2805 2810\n",
            "2810 2815\n",
            "2815 2820\n",
            "2820 2825\n",
            "2825 2830\n",
            "2830 2835\n",
            "2835 2840\n",
            "2840 2845\n",
            "2845 2850\n",
            "2850 2855\n",
            "2855 2860\n",
            "2860 2865\n",
            "2865 2870\n",
            "2870 2875\n",
            "2875 2880\n",
            "2880 2885\n",
            "2885 2890\n",
            "2890 2895\n",
            "2895 2900\n",
            "2900 2905\n",
            "2905 2910\n",
            "2910 2915\n",
            "2915 2920\n",
            "2920 2925\n",
            "2925 2930\n",
            "2930 2935\n",
            "2935 2940\n",
            "2940 2945\n",
            "2945 2950\n",
            "2950 2955\n",
            "2955 2960\n",
            "2960 2965\n",
            "2965 2970\n",
            "2970 2975\n",
            "2975 2980\n",
            "2980 2985\n",
            "2985 2990\n",
            "2990 2995\n",
            "2995 3000\n",
            "3000 3005\n",
            "3005 3010\n",
            "3010 3015\n",
            "3015 3020\n",
            "3020 3025\n",
            "3025 3030\n",
            "3030 3035\n",
            "3035 3040\n",
            "3040 3045\n",
            "3045 3050\n",
            "3050 3055\n",
            "3055 3060\n",
            "3060 3065\n",
            "3065 3070\n",
            "3070 3075\n",
            "3075 3080\n",
            "3080 3085\n",
            "3085 3090\n",
            "3090 3095\n",
            "3095 3100\n",
            "3100 3105\n",
            "3105 3110\n",
            "3110 3115\n",
            "3115 3120\n",
            "3120 3125\n",
            "3125 3130\n",
            "3130 3135\n",
            "3135 3140\n",
            "3140 3145\n",
            "3145 3150\n",
            "3150 3155\n",
            "3155 3160\n",
            "3160 3165\n",
            "3165 3170\n",
            "3170 3175\n",
            "3175 3180\n",
            "3180 3185\n",
            "3185 3190\n",
            "3190 3195\n",
            "3195 3200\n",
            "3200 3205\n",
            "3205 3210\n",
            "3210 3215\n",
            "3215 3220\n",
            "3220 3225\n",
            "3225 3230\n",
            "3230 3235\n",
            "3235 3240\n",
            "3240 3245\n",
            "3245 3250\n",
            "3250 3255\n",
            "3255 3260\n",
            "3260 3265\n",
            "3265 3270\n",
            "3270 3275\n",
            "3275 3280\n",
            "3280 3285\n",
            "3285 3290\n",
            "3290 3295\n",
            "3295 3300\n",
            "3300 3305\n",
            "3305 3310\n",
            "3310 3315\n",
            "3315 3320\n",
            "3320 3325\n",
            "3325 3330\n",
            "3330 3335\n",
            "3335 3340\n",
            "3340 3345\n",
            "3345 3350\n",
            "3350 3355\n",
            "3355 3360\n",
            "3360 3365\n",
            "3365 3370\n",
            "3370 3375\n",
            "3375 3380\n",
            "3380 3385\n",
            "3385 3390\n",
            "3390 3395\n",
            "3395 3400\n",
            "3400 3405\n",
            "3405 3410\n",
            "3410 3415\n",
            "3415 3420\n",
            "3420 3425\n",
            "3425 3430\n",
            "3430 3435\n",
            "3435 3440\n",
            "3440 3445\n",
            "3445 3450\n",
            "3450 3455\n",
            "3455 3460\n",
            "3460 3465\n",
            "3465 3470\n",
            "3470 3475\n",
            "3475 3480\n",
            "3480 3485\n",
            "3485 3490\n",
            "3490 3495\n",
            "3495 3500\n",
            "3500 3505\n",
            "3505 3510\n",
            "3510 3515\n",
            "3515 3520\n",
            "3520 3525\n",
            "3525 3530\n",
            "3530 3535\n",
            "3535 3540\n",
            "3540 3545\n",
            "3545 3550\n",
            "3550 3555\n",
            "3555 3560\n",
            "3560 3565\n",
            "3565 3570\n",
            "3570 3575\n",
            "3575 3580\n",
            "3580 3585\n",
            "3585 3590\n",
            "3590 3595\n",
            "3595 3600\n",
            "3600 3605\n",
            "3605 3610\n",
            "3610 3615\n",
            "3615 3620\n",
            "3620 3625\n",
            "3625 3630\n",
            "3630 3635\n",
            "3635 3640\n",
            "3640 3645\n",
            "3645 3650\n",
            "3650 3655\n",
            "3655 3660\n",
            "3660 3665\n",
            "3665 3670\n",
            "3670 3675\n",
            "3675 3680\n",
            "3680 3685\n",
            "3685 3690\n",
            "3690 3695\n",
            "3695 3700\n",
            "3700 3705\n",
            "3705 3710\n",
            "3710 3715\n",
            "3715 3720\n",
            "3720 3725\n",
            "3725 3730\n",
            "3730 3735\n",
            "3735 3740\n",
            "3740 3745\n",
            "3745 3750\n",
            "3750 3755\n",
            "3755 3760\n",
            "3760 3765\n",
            "3765 3770\n",
            "3770 3775\n",
            "3775 3780\n",
            "3780 3785\n",
            "3785 3790\n",
            "3790 3795\n",
            "3795 3800\n",
            "3800 3805\n",
            "3805 3810\n",
            "3810 3815\n",
            "3815 3820\n",
            "3820 3825\n",
            "3825 3830\n",
            "3830 3835\n",
            "3835 3840\n",
            "3840 3845\n",
            "3845 3850\n",
            "3850 3855\n",
            "3855 3860\n",
            "3860 3865\n",
            "3865 3870\n",
            "3870 3875\n",
            "3875 3880\n",
            "3880 3885\n",
            "3885 3890\n",
            "3890 3895\n",
            "3895 3900\n",
            "3900 3905\n",
            "3905 3910\n",
            "3910 3915\n",
            "3915 3920\n",
            "3920 3925\n",
            "3925 3930\n",
            "3930 3935\n",
            "3935 3940\n",
            "3940 3945\n",
            "3945 3950\n",
            "3950 3955\n",
            "3955 3960\n",
            "3960 3965\n",
            "3965 3970\n",
            "3970 3975\n",
            "3975 3980\n",
            "3980 3985\n",
            "3985 3990\n",
            "3990 3995\n",
            "3995 4000\n",
            "4000 4005\n",
            "4005 4010\n",
            "4010 4015\n",
            "4015 4020\n",
            "4020 4025\n",
            "4025 4030\n",
            "4030 4035\n",
            "4035 4040\n",
            "4040 4045\n",
            "4045 4050\n",
            "4050 4055\n",
            "4055 4060\n",
            "4060 4065\n",
            "4065 4070\n",
            "4070 4075\n",
            "4075 4080\n",
            "4080 4085\n",
            "4085 4090\n",
            "4090 4095\n",
            "4095 4100\n",
            "4100 4105\n",
            "4105 4110\n",
            "4110 4115\n",
            "4115 4120\n",
            "4120 4125\n",
            "4125 4130\n",
            "4130 4135\n",
            "4135 4140\n",
            "4140 4145\n",
            "4145 4150\n",
            "4150 4155\n",
            "4155 4160\n",
            "4160 4165\n",
            "4165 4170\n",
            "4170 4175\n",
            "4175 4180\n",
            "4180 4185\n",
            "4185 4190\n",
            "4190 4195\n",
            "4195 4200\n",
            "4200 4205\n",
            "4205 4210\n",
            "4210 4215\n",
            "4215 4220\n",
            "4220 4225\n",
            "4225 4230\n",
            "4230 4235\n",
            "4235 4240\n",
            "4240 4245\n",
            "4245 4250\n",
            "4250 4255\n",
            "4255 4260\n",
            "4260 4265\n",
            "4265 4270\n",
            "4270 4275\n",
            "4275 4280\n",
            "4280 4285\n",
            "4285 4290\n",
            "4290 4295\n",
            "4295 4300\n",
            "4300 4305\n",
            "4305 4310\n",
            "4310 4315\n",
            "4315 4320\n",
            "4320 4325\n",
            "4325 4330\n",
            "4330 4335\n",
            "4335 4340\n",
            "4340 4345\n",
            "4345 4350\n",
            "4350 4355\n",
            "4355 4360\n",
            "4360 4365\n",
            "4365 4370\n",
            "4370 4375\n",
            "4375 4380\n",
            "4380 4385\n",
            "4385 4390\n",
            "4390 4395\n",
            "4395 4400\n",
            "4400 4405\n",
            "4405 4410\n",
            "4410 4415\n",
            "4415 4420\n",
            "4420 4425\n",
            "4425 4430\n",
            "4430 4435\n",
            "4435 4440\n",
            "4440 4445\n",
            "4445 4450\n",
            "4450 4455\n",
            "4455 4460\n",
            "4460 4465\n",
            "4465 4470\n",
            "4470 4475\n",
            "4475 4480\n",
            "4480 4485\n",
            "4485 4490\n",
            "4490 4495\n",
            "4495 4500\n",
            "Tensor([[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]], requires_grad=True)\n",
            "Tensor(nan, requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# 3. start training and have fun!\n",
        "print(model.fc3.weight)\n",
        "print(model.fc3.bias)\n",
        "\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "for epoch in range(1):\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for start in range(0, 4500, batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        end = start + batch_size\n",
        "        print(start, end)\n",
        "\n",
        "        inputs = train[start:end]\n",
        "\n",
        "\n",
        "        # TODO: predicted\n",
        "        predicted = model.forward(inputs)\n",
        "\n",
        "        actual = train_label[start:end]\n",
        "        actual.data = actual.data.reshape(batch_size, 1)\n",
        "        # TODO: calcualte MSE loss\n",
        "        loss =MeanSquaredError(predicted, actual)\n",
        "\n",
        "\n",
        "        # TODO: backward\n",
        "        # hint you need to just do loss.backward()\n",
        "        loss.backward()\n",
        "        # print(\"new params\", params)\n",
        "        # TODO: add loss to epoch_loss\n",
        "        epoch_loss += loss\n",
        "\n",
        "\n",
        "        # TODO: update w and b using optimizer.step()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "# TODO: print weight and bias of linear layer\n",
        "\n",
        "print(model.fc3.weight)\n",
        "print(model.fc3.bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "yBIYlKlDFLSz",
        "outputId": "ecd437f3-a7d1-4b3c-c86e-fad540c5dfd2"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-7e00038e32d0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'fc1'"
          ]
        }
      ],
      "source": [
        "print(model.fc1.weight)\n",
        "print(model.fc1.bias)\n",
        "\n",
        "print(model.fc2.weight)\n",
        "print(model.fc2.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9kfiimSDxAU",
        "outputId": "3c58ef48-29c9-420d-dc9d-acab8a3b9bb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor([[nan nan nan nan nan nan nan nan nan nan]], requires_grad=True)\n",
            "Tensor(7, requires_grad=False)\n"
          ]
        }
      ],
      "source": [
        "pred = model.forward(test[0])\n",
        "print(pred)\n",
        "print(test_label[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "9f8f6ac5df62d4c010a470c096af89c29157a9d743a2d25d86d4ae7e90eff14c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}